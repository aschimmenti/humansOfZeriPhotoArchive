{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fc4ad9",
   "metadata": {},
   "source": [
    "To investigate and extract all the photographers partaking in the Zeri Photo Archive we started with some exploratory queries.  We saw that the photographs were linked to their creators by a recursive relation, such that ?photo crm:P94i_was_created_by ?creation . ?creation crm:P14_carried_out_by ?photographer. \n",
    "The definition of \"Photographer\" in this context was \"an entity that carried out a creation process which created the resource\". This was our first successful query  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e735a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_SPARQL_query = \"\"\"\n",
    "PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?photographer  \n",
    "WHERE { \n",
    "  \t?x rdf:type <http://www.essepuntato.it/2014/03/fentry/Photograph> ; \n",
    "    crm:P94i_was_created_by ?creation .\n",
    "    ?creation crm:P14_carried_out_by ?photographer .\n",
    " }\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66acc2c",
   "metadata": {},
   "source": [
    "Once we found the photographers we wanted to count the contributions of each one of them made to the Zeri Archive, and we saw that the property <http://purl.org/spar/pro/holdsRoleInTime> was repeated for each photo they created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0651909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the libraries we need at once: \n",
    "import rdflib\n",
    "from rdflib import Namespace\n",
    "from rdflib.namespace import DCTERMS\n",
    "from rdflib.namespace import RDFS\n",
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import XSD\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, GET\n",
    "import csv \n",
    "import pandas as pd\n",
    "from json import decoder\n",
    "import requests\n",
    "import json\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ca8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# get the endpoint API\n",
    "fototeca_endpoint = \"http://data.fondazionezeri.unibo.it/sparql\"\n",
    "\n",
    "# prepare the query : 10 random triples\n",
    "my_SPARQL_query = \"\"\"\n",
    "PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?photographer_label (COUNT(<http://purl.org/spar/pro/holdsRoleInTime>) as ?cnt)\n",
    "WHERE { \n",
    "  \t?x rdf:type <http://www.essepuntato.it/2014/03/fentry/Photograph> ; \n",
    "    crm:P94i_was_created_by ?creation .\n",
    "    ?creation crm:P14_carried_out_by ?photographer .\n",
    "    ?photographer rdfs:label ?photographer_label\n",
    " }\n",
    "GROUP BY ?photographer_label \n",
    "ORDER BY DESC(?cnt) ?photographer_label\n",
    "\"\"\"\n",
    "\n",
    "# set the endpoint \n",
    "sparql_ft = SPARQLWrapper(fototeca_endpoint)\n",
    "# set the query\n",
    "sparql_ft.setQuery(my_SPARQL_query)\n",
    "# set the returned format\n",
    "sparql_ft.setReturnFormat(JSON)\n",
    "# get the results\n",
    "results = sparql_ft.query().convert()\n",
    "\n",
    "with open('photographers.csv', mode='w') as my_file:\n",
    "    my_writer = csv.writer(my_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    # write the column names\n",
    "    my_writer.writerow(['photographer', 'contribution count'])\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        my_writer.writerow([result['photographer_label']['value'], result['cnt']['value'].strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d33674",
   "metadata": {},
   "source": [
    "Looking at the data we see that most of the photographies inside the Zeri Archive are anonymous, and the following four entities are not, apparently, individual people as we first thought. We need now to find each one of the photographers inside another knowledge base to have more informations about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead486d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photographer</th>\n",
       "      <th>contribution count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anonimo</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brogi</td>\n",
       "      <td>2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Istituto Centrale per il Catalogo e la Documen...</td>\n",
       "      <td>1539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alinari, Fratelli</td>\n",
       "      <td>1532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anderson</td>\n",
       "      <td>1192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        photographer  contribution count\n",
       "0                                            Anonimo               13355\n",
       "1                                              Brogi                2213\n",
       "2  Istituto Centrale per il Catalogo e la Documen...                1539\n",
       "3                                  Alinari, Fratelli                1532\n",
       "4                                           Anderson                1192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"photographers.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae37b6a",
   "metadata": {},
   "source": [
    "We decided to find more informations on Wikidata, so the first need was to find each Wikidata ID (this time using the .json format) for the entities identifiable as photographers according to the contextual definition of the Zeri Archive. We managed to do that via the Wikidata API. \n",
    "This function saves all the API responses, since many searches yielded more than one result; in the API response there was a key ['search-continue'] which allowed the function to continue to append new findings to the results list. \n",
    "In practice, if there is more than one \"James Anderson\", all of them will be saved to the outcomes, since we don't know if the first \"James Anderson\" is exactly who we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95622222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from json import decoder\n",
    "from SPARQLWrapper.Wrapper import GET\n",
    "import requests\n",
    "import json\n",
    "import rdflib\n",
    "import pprint\n",
    "from rdflib import Namespace\n",
    "from rdflib.namespace import DCTERMS\n",
    "from rdflib.namespace import RDFS\n",
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import XSD\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from SPARQLWrapper import SPARQLWrapper, POST, DIGEST, JSON\n",
    "from SPARQLWrapper import POST \n",
    "import ssl\n",
    "import csv\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#do not use\n",
    "def to_text(path):\n",
    "    final_text = list()\n",
    "    with open(path, newline='') as csvfile:\n",
    "        photographers = csv.DictReader(csvfile)\n",
    "        for row in photographers:\n",
    "            x = str(row['photographer']) \n",
    "            y = int(row['contribution count'])\n",
    "            text = list()\n",
    "            for n in range(y):\n",
    "                text.append(x)\n",
    "            final_text.append(' '.join(text))\n",
    "    final_string = ' '.join(final_text)\n",
    "    return final_string\n",
    "        \n",
    "def save_to_file(content, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def reverse_string(string): \n",
    "    comma = ', '\n",
    "    string_to_join = ''\n",
    "    if comma in string: \n",
    "        x = string.split(\", \")\n",
    "        string_to_join = str(x[1]) + ' '+ str(x[0])\n",
    "        return string_to_join\n",
    "    else: \n",
    "        return string\n",
    "\n",
    "name_file = open('fototeca_photographers.json') \n",
    "base_url = \"https://www.wikidata.org/w/api.php?action=wbsearchentities&search=%s&language=en&format=json&limit=50\"\n",
    "\n",
    "data = json.load(name_file)\n",
    "dict_of_results = {}\n",
    "list_of_conceptualuris = []\n",
    "\n",
    "for idx, row in enumerate(data[\"results\"][\"bindings\"]):\n",
    "    search_string = row[\"photographer_label\"][\"value\"]\n",
    "    search_string = reverse_string(search_string)\n",
    "    final_str =  ('+'.join(search_string.split(' '))).strip()\n",
    "    search_res = requests.get( base_url % final_str).json()\n",
    "    n_results = len(search_res['search'])\n",
    "    if(n_results == 0):\n",
    "        continue\n",
    "\n",
    "    search_results = []\n",
    "    search_results.extend(search_res['search'])\n",
    "    \n",
    "    if('search-continue' in search_res.keys()):\n",
    "        any_remaining_data = True\n",
    "        continue_val = 1\n",
    "        while(any_remaining_data):\n",
    "            new_results = requests.get((base_url + ('&continue=%i'%continue_val)) % final_str).json()\n",
    "            search_results.extend(new_results['search'])\n",
    "            any_remaining_data ='search-continue' in  new_results.keys()\n",
    "            continue_val += 1\n",
    "    for s in search_results:\n",
    "            list_of_conceptualuris.append(s['concepturi'])\n",
    "\n",
    "def suit_for_SPARQL_dinner(list_of_uris): \n",
    "    bracketed_uris = []\n",
    "    for uri in list_of_uris:\n",
    "        suited_uri = '<' + uri + '>'\n",
    "        bracketed_uris.append(suited_uri)\n",
    "    return bracketed_uris\n",
    "\n",
    "\n",
    "uris = ' '.join(suit_for_SPARQL_dinner(list_of_conceptualuris))\n",
    "\n",
    "def save_to_file(content, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "save_to_file(uris, \"uris3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa194f",
   "metadata": {},
   "source": [
    "Let's prepare the uris to be finally queried on Wikidata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper.Wrapper import POST\n",
    "import rdflib\n",
    "from rdflib import Namespace\n",
    "from rdflib.namespace import DCTERMS\n",
    "from rdflib.namespace import RDFS\n",
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import XSD\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, GET, POST\n",
    "import csv \n",
    "import pandas as pd\n",
    "from json import decoder\n",
    "import requests\n",
    "import ssl\n",
    "import json \n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\")\n",
    "sparql.setMethod(POST)\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "#adds the uris from a sparql query to a list, its best use is with the function below \n",
    "def invitation_list(data, string_to_match):\n",
    "    invitated_uris = set()\n",
    "    for result in data[\"results\"][\"bindings\"]:\n",
    "        invitated_uris.add(result[string_to_match][\"value\"]) \n",
    "        uris = list(invitated_uris)\n",
    "    return uris\n",
    "\n",
    "#once you have the list you can: \n",
    "#clean the list (first below)\n",
    "#suit the uris with the brackets (second below)\n",
    "#takes a list of uris from a query and adds brackets for a sparql query \n",
    "\n",
    "def suit_for_SPARQL_dinner(list_of_uris): \n",
    "    bracketed_uris = []\n",
    "    for uri in list_of_uris:\n",
    "        suited_uri = '<' + uri + '>'\n",
    "        bracketed_uris.append(suited_uri)\n",
    "    return bracketed_uris\n",
    "\n",
    "#if you need to remove uris from a list, it's a basic linear search \n",
    "def remove_uninvited_guests_from_list(uninvited_guests, invitation_list):\n",
    "    final_set = set()\n",
    "    if len(invitation_list) < 2: \n",
    "        return None\n",
    "    for person in uninvited_guests: \n",
    "        for i in range(len(invitation_list)):\n",
    "            if (person == invitation_list[i]): \n",
    "                print('got out')\n",
    "                print(person)\n",
    "            else: \n",
    "                final_set.add(invitation_list[i])\n",
    "    exclusive_list = list(final_set) \n",
    "    return exclusive_list\n",
    "\n",
    "def afterparty_trash(filename, data_to_write):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data_to_write, outfile)\n",
    "##################################################################################\n",
    "\n",
    "name_file = open('py_files/json_files/final_photographer_Q5.json')\n",
    "data = json.load(name_file)\n",
    "uris = suit_for_SPARQL_dinner(invitation_list(data, \"photographer\"))\n",
    "string_uris = ' '.join(uris)\n",
    "\n",
    "#first query: find out if there's some group of people that isn't a Q5 themselves\n",
    "#if there's someone, check if there's some people related to them and update the list of uris\n",
    "\n",
    "if_entity = \"\"\"select ?otherpeople ?photographer\n",
    "where {VALUES  ?photographer  {\"\"\"+string_uris+\"\"\"}\n",
    "    ?photographer wdt:P31 ?o .\n",
    "    FILTER(?o != wd:Q5) . \n",
    "    ?photographer rdfs:label ?label .\n",
    "    FILTER(LANG(?label) = \"en\").  \n",
    "    ?photographer ?property ?otherpeople .\n",
    "    ?otherpeople wdt:P31 wd:Q5 .\n",
    "}\n",
    "\n",
    "GROUP BY ?otherpeople ?photographer\"\"\"\n",
    "\n",
    "sparql.setQuery(if_entity)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "\n",
    "#now add to the uris the list of new uris, first make a list of them, then 'suit them' with the brackets\n",
    "uris.extend(suit_for_SPARQL_dinner(invitation_list(results, \"otherpeople\")))\n",
    "#create a second list out of the other output, and have a list of the people to remove from the now unpacked list\n",
    "people_out = suit_for_SPARQL_dinner(invitation_list(results, 'photographer'))\n",
    "#use the two previous lists \n",
    "new_uris = remove_uninvited_guests_from_list(people_out, uris)\n",
    "new_string_uris = ' '.join(new_uris)\n",
    "##############################################\n",
    "print(new_string_uris)\n",
    "\n",
    "#now let's see the citizenships of the new uris selected\n",
    "citizenships_query= \"\"\"\n",
    "select ?photographer ?label (group_concat(?citizenship) as ?citizenships) ?worklocation\n",
    "where {VALUES ?photographer {\"\"\" + new_string_uris + \"\"\"}\n",
    "    ?photographer rdfs:label ?label .\n",
    "    FILTER(LANG(?label) = \"en\").  \n",
    "       optional {\n",
    "          ?photographer wdt:P27 ?citizenship\n",
    "}\n",
    "}\n",
    "group by ?photographer ?label\n",
    "\"\"\"\n",
    "sparql.setQuery(citizenships_query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "afterparty_trash('py_files/json_files/citizenships.json', results)\n",
    "\n",
    "#let's check the dates related to the new uris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import defpath\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "from collections import defaultdict\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.readwrite.json_graph.node_link import node_link_data\n",
    "from networkx.readwrite import json_graph;\n",
    "from itertools import product\n",
    "\n",
    "def afterparty_trash(filename, data_to_write):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data_to_write, outfile)\n",
    "\n",
    "G = nx.MultiGraph()\n",
    "name_file = open('json_files/worklocations.json')\n",
    "data = json.load(name_file)\n",
    "color_map = []\n",
    "graphdict = defaultdict()\n",
    "\n",
    "for row in data[\"results\"][\"bindings\"]:\n",
    "    if \"worklabel\" in row.keys():  \n",
    "        newk = row[\"worklabel\"][\"value\"]\n",
    "        if newk not in graphdict.keys():\n",
    "            graphdict[newk] = list()\n",
    "\n",
    "for row in data[\"results\"][\"bindings\"]:\n",
    "    if \"worklabel\" in row.keys(): \n",
    "        newv =  row['label']['value']\n",
    "        city = row['worklabel']['value']\n",
    "        citizenship = row['worklabel']['value']\n",
    "        if city in graphdict.keys():\n",
    "            graphdict[city].append(newv)\n",
    "print(graphdict)\n",
    "\n",
    "for key in graphdict: \n",
    "    G.add_node(key, vote=\"city\")\n",
    "    print(key)\n",
    "    for value in graphdict[key]:\n",
    "        G.add_node(value, vote=\"person\")\n",
    "        G.add_edge(key, value)\n",
    "        \n",
    "\n",
    "color_map = []\n",
    "for node, data in G.nodes(data=True):\n",
    "    if data['vote'] == 'city':\n",
    "        color_map.append(0.25)  # blue color\n",
    "    elif data['vote'] == 'person':\n",
    "        color_map.append(0.7)  # yellow color\n",
    "\n",
    "nx.draw(G, vmin=0, vmax=1, cmap=plt.cm.jet, node_color=color_map, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "#with open('graph.json', 'w') as outfile:\n",
    "    #json.dump(json_graph.node_link_data(G))\n",
    "#x = json_graph.node_link_data(G)\n",
    "#afterparty_trash('graph.json', x)\n",
    "\n",
    "    \n",
    "#italian cities with >= 4 are: Rome, Bologna, Milan, Florence, Venice\n",
    "higher_n_cities = defaultdict()\n",
    "italian_cities = ['Rome','Bologna','Milan','Florence','Venice']\n",
    "for c in italian_cities:\n",
    "    higher_n_cities[c] = list()\n",
    "\n",
    "IC = nx.MultiGraph()\n",
    "\n",
    "for key in graphdict:  \n",
    "    for key2 in higher_n_cities:\n",
    "        if key == key2: \n",
    "            higher_n_cities[key2] = graphdict[key].copy()\n",
    "\n",
    "print(higher_n_cities)\n",
    "\n",
    "for key in higher_n_cities: \n",
    "    IC.add_node(key, vote=\"city\")\n",
    "    for value in higher_n_cities[key]:\n",
    "        IC.add_node(value, vote=\"person\")\n",
    "        IC.add_edge(key, value)\n",
    "\n",
    "\n",
    "color_map = []\n",
    "for node, data in IC.nodes(data=True):\n",
    "    if data['vote'] == 'city':\n",
    "        color_map.append(0.25)  # blue color\n",
    "    elif data['vote'] == 'person':\n",
    "        color_map.append(0.7)  # yellow color        \n",
    "\n",
    "\n",
    "nx.draw(IC, vmin=0, vmax=1, cmap=plt.cm.jet, node_color=color_map, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "FR = nx.MultiGraph()\n",
    "\n",
    "freqs = {\"People and organizations\": 10,\"Artists, schools, periods\": 17,\"Genres and themes\":11}\n",
    "\n",
    "for key, value in freqs.items():  \n",
    "    FR.add_node(key)\n",
    "    FR.add_node(value)\n",
    "    FR.add_edge(key, value)\n",
    "\n",
    "nx.draw(FR, with_labels=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}